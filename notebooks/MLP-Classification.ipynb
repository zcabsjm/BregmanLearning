{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb3caf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various torch packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# ------------------------\n",
    "# get up one directory \n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "# ------------------------\n",
    "\n",
    "# custom packages\n",
    "import models.aux_funs as maf\n",
    "import optimizers as op\n",
    "import regularizers as reg\n",
    "import train\n",
    "import math\n",
    "import utils.configuration as cf\n",
    "import utils.datasets as ud\n",
    "from models.fully_connected import fully_connected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc1ce83",
   "metadata": {},
   "source": [
    "# Fix the random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db27eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 2\n",
    "cf.seed_torch(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aebc6f",
   "metadata": {},
   "source": [
    "# Configure the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3dcac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_init = 0.01\n",
    "r = [1,0.7/math.sqrt(sparse_init)]\n",
    "\n",
    "conf_args = {#\n",
    "    # data specification\n",
    "    'data_file':\"../../Data\", 'train_split':0.80, 'data_set':\"MNIST\", 'download':True,\n",
    "    # cuda\n",
    "    'use_cuda':False, 'num_workers':0, 'cuda_device':0, 'pin_memory':True, 'train_split':0.80,\n",
    "    #\n",
    "    'epochs':7,\n",
    "    # optimizer\n",
    "    'delta':1.0, 'lr':0.1, 'lamda':1e-1, 'optim':\"LinBreg\",'beta':0.0,\n",
    "    # initialization\n",
    "    'sparse_init':sparse_init, 'r':r,\n",
    "    # misc\n",
    "    'random_seed':random_seed, 'eval_acc':True,\n",
    "}\n",
    "\n",
    "conf = cf.Conf(**conf_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17174855",
   "metadata": {},
   "source": [
    "# Initiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfa6692",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {'mean':conf.data_set_mean, 'std':conf.data_set_std}    \n",
    "\n",
    "# Determine input size based on dataset\n",
    "if conf.data_set == \"CIFAR10\":\n",
    "    input_size = 3072  # 32x32x3\n",
    "elif conf.data_set in [\"MNIST\", \"Fashion-MNIST\"]:\n",
    "    input_size = 784   # 28x28x1\n",
    "else:\n",
    "    input_size = 784  # Default fallback\n",
    "    \n",
    "sizes = [input_size, 200, 80, 10]  # Dynamically set input size\n",
    "act_fun = torch.nn.ReLU()\n",
    "    \n",
    "model = fully_connected(sizes, act_fun, **model_kwargs)\n",
    "best_model = train.best_model(fully_connected(sizes, act_fun, **model_kwargs).to(conf.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096f66c7",
   "metadata": {},
   "source": [
    "# Weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991a91ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {'mean':conf.data_set_mean, 'std':conf.data_set_std}    \n",
    "def init_weights(conf, model):\n",
    "    # sparsify\n",
    "    maf.sparse_bias_uniform_(model, 0,conf.r[0])\n",
    "    maf.sparse_weight_normal_(model, conf.r[1])\n",
    "    \n",
    "    maf.sparsify_(model, conf.sparse_init)\n",
    "    model = model.to(conf.device)\n",
    "    return model\n",
    "\n",
    "model = init_weights(conf,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e96961d",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980c3413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_opt(conf, model):\n",
    "    weights_linear = maf.get_weights_linear(model)\n",
    "    biases = maf.get_bias(model)\n",
    "\n",
    "    if conf.optim == \"SGD\":\n",
    "        opt = torch.optim.SGD(model.parameters(), lr=conf.lr, momentum=conf.beta)\n",
    "    elif conf.optim == \"LinBreg\":\n",
    "        opt = op.LinBreg([{'params': weights_linear, 'lr' : conf.lr, 'reg' : reg.reg_l1(lamda=conf.lamda), 'momentum':conf.beta, 'delta':conf.delta},\n",
    "                          {'params': biases, 'lr': conf.lr, 'momentum':conf.beta}])\n",
    "    elif conf.optim == \"LinBregHeavyBall\":\n",
    "        opt = op.LinBregHeavyBall([{'params': weights_linear, 'lr': conf.lr,\n",
    "                                    'reg': reg.reg_l1(lamda=conf.lamda), 'momentum': conf.beta,\n",
    "                                    'delta': conf.delta},\n",
    "                                   {'params': biases, 'lr': conf.lr, 'momentum': conf.beta}])\n",
    "    elif conf.optim == \"LinBregNesterov\":\n",
    "        opt = op.LinBregNesterov([{'params': weights_linear, 'lr': conf.lr,\n",
    "                                  'reg': reg.reg_l1(lamda=conf.lamda), 'momentum': conf.beta,\n",
    "                                  'delta': conf.delta},\n",
    "                                 {'params': biases, 'lr': conf.lr, 'momentum': conf.beta}])\n",
    "    elif conf.optim == \"adam\":\n",
    "        opt = op.AdaBreg([{'params': weights_linear, 'lr' : conf.lr, 'reg' : reg.reg_l1(lamda=conf.lamda)},\n",
    "                          {'params': biases, 'lr': conf.lr}])\n",
    "    elif conf.optim == \"ProxSGD\":\n",
    "        opt = op.ProxSGD([{'params': weights_linear, 'lr' : conf.lr, 'reg' : reg.reg_l1(lamda=conf.lamda)},\n",
    "                          {'params': biases, 'lr': conf.lr}])\n",
    "    else:\n",
    "        raise ValueError(\"Unknown Optimizer specified\")\n",
    "\n",
    "    # learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.5, patience=5,threshold=0.01)\n",
    "    \n",
    "    return opt, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff6e6da",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7617e6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = ud.get_data_set(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aed2e9",
   "metadata": {},
   "source": [
    "# History and Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0124dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize history\n",
    "tracked = ['acc', 'loss', 'linear_sparse', 'reg_vals']\n",
    "\n",
    "def reset_hist(tracked):\n",
    "    train_hist = {}\n",
    "    val_hist = {}\n",
    "    return train_hist, val_hist\n",
    "\n",
    "params = [\n",
    "    # SGD Run (LinBreg with lamda=0.0, beta=0.0)\n",
    "    {'optim': 'LinBreg', 'reps': 1, 'lamda': 0.0, 'random_seed': 0, 'label': 'SGD'},\n",
    "\n",
    "    # LinBreg Run (beta=0.0)\n",
    "    {'optim': 'LinBreg', 'reps': 1, 'lamda': 1e-1, 'beta': 0.0, 'random_seed': 0, 'label': 'LinBreg ($\\lambda=1$e-1)'},\n",
    "\n",
    "    # LinBregNesterov Run (beta=0.9)\n",
    "    {'optim': 'LinBregNesterov', 'reps': 1, 'lamda': 1e-1, 'beta': 0.5, 'random_seed': 0, 'label': 'LinBregNesterov ($\\lambda=1$e-3, $\\\\beta=0.5$)'},\n",
    "\n",
    "    # AdaBreg Run\n",
    "    {'optim': 'adam', 'reps': 1, 'lamda': 1e-1, 'random_seed': 0, 'label': 'AdaBreg ($\\\\lambda=1$e-1)'},\n",
    "\n",
    " ]\n",
    "runs = cf.run(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bf1e65",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49686c02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while runs.step(conf):\n",
    "    # -----------------------------------------------------------------------------------\n",
    "    # Reinit weights and the corresponding optimizer\n",
    "    # -----------------------------------------------------------------------------------\n",
    "    train_hist, val_hist = reset_hist(tracked)\n",
    "    model = init_weights(conf, model)\n",
    "    opt, scheduler = init_opt(conf, model)\n",
    "    \n",
    "    # -----------------------------------------------------------------------------------\n",
    "    # train the model\n",
    "    # -----------------------------------------------------------------------------------\n",
    "    for epoch in range(conf.epochs):\n",
    "        print(25*\"<>\")\n",
    "        print(50*\"|\")\n",
    "        print(25*\"<>\")\n",
    "        print('Epoch:', epoch)\n",
    "\n",
    "        # ------------------------------------------------------------------------\n",
    "        # train step, log the accuracy and loss\n",
    "        # ------------------------------------------------------------------------\n",
    "        train_data = train.train_step(conf, model, opt, train_loader)\n",
    "\n",
    "        # update history\n",
    "        for key in tracked:\n",
    "            if key in train_data:\n",
    "                var_list = train_hist.setdefault(key, [])\n",
    "                var_list.append(train_data[key])        \n",
    "\n",
    "        # ------------------------------------------------------------------------\n",
    "        # validation step\n",
    "        val_data = train.validation_step(conf, model, opt, valid_loader)\n",
    "\n",
    "        # update history\n",
    "        for key in tracked:\n",
    "            \n",
    "            \n",
    "            if key in val_data:\n",
    "                var = val_data[key]\n",
    "                if isinstance(var, list):\n",
    "                    for i, var_loc in enumerate(var):\n",
    "                        key_loc = key+\"_\" + str(i)\n",
    "                        var_list = val_hist.setdefault(key_loc, [])\n",
    "                        val_hist[key_loc].append(var_loc)\n",
    "                else:\n",
    "                    var_list = val_hist.setdefault(key, [])\n",
    "                    var_list.append(var)    \n",
    "\n",
    "        # scheduler step\n",
    "        scheduler.step(train_data['loss'])\n",
    "        print(\"Learning rate:\",opt.param_groups[0]['lr'])\n",
    "        \n",
    "        # update beset model\n",
    "        best_model(train_data['acc'], val_data['acc'], model=model)\n",
    "\n",
    "        \n",
    "    # add values to the run history\n",
    "    runs.add_history(train_hist, \"train\")\n",
    "    runs.add_history(val_hist, \"val\")\n",
    "            \n",
    "    # update random seed\n",
    "    cf.seed_torch(conf.random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9ea1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if runs.history:\n",
    "    print(\"Keys available in the first run's history:\")\n",
    "    print(runs.history[0].keys())\n",
    "else:\n",
    "    print(\"No history recorded yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b06121",
   "metadata": {},
   "source": [
    "# Prepare Data\n",
    "In this step we average over different runs of the same parameter configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d610069",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "hist = runs.history\n",
    "keys = ['val_acc', 'val_linear_sparse']\n",
    "hist_idx = 0\n",
    "for param in params:\n",
    "    data = {}\n",
    "    for key in keys:\n",
    "        if not key in hist[hist_idx]:\n",
    "            continue\n",
    "        \n",
    "        if key == 'train_acc' or key == 'val_acc' or key == 'val_linear_sparse':\n",
    "            rescale = 100\n",
    "        else:\n",
    "            rescale = 1/param['lamda'] if param['lamda'] > 0.0 else 0.0\n",
    "            \n",
    "        n = len(hist[hist_idx][key])\n",
    "        m = param.get('reps',1)\n",
    "        data_loc = np.zeros(shape=(n,m))\n",
    "        \n",
    "        # assign data and save it into local array for mean and average\n",
    "        for i in range(m):\n",
    "            var = np.array(hist[hist_idx + i][key])\n",
    "            data_loc[:,i] = rescale*var\n",
    "            data[key+\"_run_\" + str(i)] = rescale*var\n",
    "\n",
    "        # mean and std of the data\n",
    "        data[key+\"_mean\"] = np.mean(data_loc,axis=1)\n",
    "        data[key+\"_std\"] = np.std(data_loc,axis=1)\n",
    "\n",
    "                # --- Add this print statement ---\n",
    "        if key == 'train_acc':\n",
    "            print(f\"  Debug: Successfully created 'train_acc_mean'. Length: {len(data[key+'_mean'])}\")\n",
    "        # --- End of added print statement ---\n",
    "        \n",
    "        param['result'] = data\n",
    "        \n",
    "        # update the history index\n",
    "    hist_idx += m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869ca35c",
   "metadata": {},
   "source": [
    "# Setup plots and appearance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964a7112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.style.use('ggplot')\n",
    "matplotlib.rcParams['mathtext.fontset'] = 'cm'\n",
    "matplotlib.rcParams['font.family'] = 'STIXGeneral'\n",
    "matplotlib.rcParams['font.size']=8\n",
    "matplotlib.rcParams['lines.linewidth'] = 1\n",
    "matplotlib.rcParams['lines.markersize'] = 2\n",
    "matplotlib.rcParams['text.color'] = 'black'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5302ab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_training_stats(ax, keys_to_plot, data_dict, label='', color='k',alpha=1.0, alpha_fill=0.2):\n",
    "    # keys_to_plot should be ['train_acc', 'val_acc', 'val_reg_vals_0', 'val_linear_sparse']\n",
    "    # data_dict is param['result']\n",
    "    \n",
    "    for i in range(len(keys_to_plot)):\n",
    "        current_key = keys_to_plot[i]\n",
    "        mean_key = current_key + '_mean'\n",
    "        std_key = current_key + '_std'\n",
    "        \n",
    "        # Debug print specifically for the first axis (train_acc)\n",
    "        if i == 0:\n",
    "            print(f\"  Plotting Function Check (ax[0], label='{label}'): Trying to plot key '{current_key}'. Looking for '{mean_key}' in data_dict.\")\n",
    "            if mean_key in data_dict:\n",
    "                 print(f\"    '{mean_key}' found. Data length: {len(data_dict[mean_key])}\")\n",
    "            else:\n",
    "                 print(f\"    '{mean_key}' NOT FOUND in data_dict!\")\n",
    "\n",
    "\n",
    "        if not mean_key in data_dict:\n",
    "            # print(f\"    Skipping {mean_key} for label '{label}' - not found in data.\") # Optional: uncomment for more verbose output\n",
    "            continue\n",
    "        if not std_key in data_dict:\n",
    "            # print(f\"    Skipping {std_key} for label '{label}' - not found in data.\") # Optional: uncomment for more verbose output\n",
    "            continue\n",
    "            \n",
    "        # --------------------------------\n",
    "        var_mean = data_dict[mean_key]\n",
    "        var_std = data_dict[std_key]\n",
    "        # --------------------------------\n",
    "        \n",
    "        # Ensure data is not empty\n",
    "        if len(var_mean) == 0:\n",
    "            print(f\"    Warning: '{mean_key}' for label '{label}' is empty. Skipping plot.\")\n",
    "            continue\n",
    "\n",
    "        epochs = np.arange(len(var_mean))\n",
    "        \n",
    "        # Check target axis index is valid\n",
    "        if i < len(ax):\n",
    "             target_ax = ax[i]\n",
    "             target_ax.plot(epochs,var_mean, label=label, color=color,alpha=alpha)\n",
    "             target_ax.fill_between(epochs, var_mean - var_std, var_mean + var_std, color=color, alpha=alpha_fill)\n",
    "        else:\n",
    "            print(f\"    Warning: Axis index {i} out of bounds for label '{label}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0596d53c",
   "metadata": {},
   "source": [
    "# Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe8cc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp = matplotlib.cm.get_cmap(name='Accent') \n",
    "\n",
    "colors = [\n",
    "    cmp(0.0), # SGD (e.g., blue)\n",
    "    cmp(0.7), # LinBreg (e.g., green)\n",
    "    'red',    # LinBregNesterov (red)\n",
    "    cmp(0.2), # AdaBreg (e.g., orange)\n",
    "]\n",
    "\n",
    "if len(colors) < len(params):\n",
    "    num_missing = len(params) - len(colors)\n",
    "    for i in range(num_missing):\n",
    "        colors.append(cmp( (len(colors) + i) * 0.1 % 1.0 )) \n",
    "\n",
    "for i, param in enumerate(params):\n",
    "    param['color'] = colors[i]\n",
    "    param.setdefault('label', param['optim'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17606bf4",
   "metadata": {},
   "source": [
    "# Final Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01240bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(24, 12))  \n",
    "ax = np.ravel(ax)  \n",
    "\n",
    "plt.rcParams['font.size'] = 26            \n",
    "plt.rcParams['axes.labelsize'] = 36       \n",
    "plt.rcParams['xtick.labelsize'] = 36      \n",
    "plt.rcParams['ytick.labelsize'] = 36      \n",
    "plt.rcParams['legend.fontsize'] = 36      \n",
    "plt.rcParams['lines.linewidth'] = 3.5     \n",
    "\n",
    "params[0]['label'] = 'SGM'  \n",
    "params[1]['label'] = 'LinBreg ($\\lambda=1$e-1)'\n",
    "params[2]['label'] = 'LinBregNesterov ($\\lambda=5$e-2, $\\\\beta=0.5$)'\n",
    "\n",
    "for param in params:\n",
    "    plot_training_stats(ax, keys, param['result'], color=param['color'], label=param['label'])\n",
    "    \n",
    "ax[0].set_ylabel('Validation Accuracy [%]', fontsize=40)  \n",
    "ax[0].set_xlabel('Epoch', fontsize=40)  \n",
    "\n",
    "ax[1].set_ylabel('Non-Zero Entries [%]', fontsize=40)  \n",
    "ax[1].set_xlabel('Epoch', fontsize=40)  \n",
    "\n",
    "ax[0].set_ylim(bottom=95.0, top=98.5)\n",
    "yticks = np.arange(95.0, 99.0, 0.5)\n",
    "ax[0].set_yticks(yticks)\n",
    "ax[0].set_yticklabels([f'{y:.1f}' for y in yticks], fontsize=30)  \n",
    "\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 1.1), \n",
    "           prop={'size': 32}, ncol=len(params))  \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.82, wspace=0.3)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e94369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "optimizer_labels = [\n",
    "    'SGD',\n",
    "    'LinBreg ($\\lambda=1$e-1)',\n",
    "    'LinBregNesterov ($\\lambda=1$e-3, $\\\\beta=0.5$)',\n",
    "    'AdaBreg ($\\lambda=1$e-1)',\n",
    "]\n",
    "\n",
    "records = []\n",
    "\n",
    "# Each run's history is stored in runs.history.\n",
    "# We assume that for each run:\n",
    "#    - run['label'] is the optimizer label (e.g., \"LinBreg ($\\lambda=1$e-3)\")\n",
    "#    - run['lamda'] is the regularization parameter value\n",
    "#    - run['train_acc'] and run['val_acc'] store the final accuracies.\n",
    "#    - run['val_linear_sparse'] stores the sparsity level.\n",
    "for i, run in enumerate(runs.history):\n",
    "    label = optimizer_labels[i % len(optimizer_labels)]  # Use modulo to handle repetitions\n",
    "    sparsity = run.get('val_linear_sparse', [None])[-1]\n",
    "    train_acc = run.get('train_acc', [None])[-1]\n",
    "    val_acc = run.get('val_acc', [None])[-1]\n",
    "    \n",
    "    records.append({\n",
    "        'Optimizer': label,\n",
    "        'Sparsity Level': sparsity,\n",
    "        'Train Accuracy': train_acc,\n",
    "        'Validation Accuracy': val_acc\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
