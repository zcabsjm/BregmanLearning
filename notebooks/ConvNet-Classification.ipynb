{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044d7fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various torch packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# ------------------------\n",
    "# get up one directory \n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "# ------------------------\n",
    "\n",
    "# custom packages\n",
    "import models.aux_funs as maf\n",
    "import optimizers as op\n",
    "import regularizers as reg\n",
    "import train\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import utils.configuration as cf\n",
    "import utils.datasets as ud\n",
    "from models.mnist_conv import mnist_conv\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from custom_regularizers import DynamicRankNuclearRegularizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febb1290",
   "metadata": {},
   "source": [
    "# Fix the random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf73f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 0\n",
    "cf.seed_torch(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce1cf2a",
   "metadata": {},
   "source": [
    "# Configure the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3dd52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_args = {#\n",
    "    # data specification\n",
    "    'data_file':\"../../datasets\",'train_split':0.95, 'data_set':\"Fashion-MNIST\", 'download':True,\n",
    "    # cuda\n",
    "    'use_cuda':False, 'num_workers':2, 'cuda_device':0, 'pin_memory':True, 'train_split':0.95,\n",
    "    #\n",
    "    'epochs':50,\n",
    "    # optimizer\n",
    "    'delta':20.0, 'lr':0.01, 'lamda_0':1e-4, 'lamda_1':0.008, 'optim':\"LinBreg\", 'conv_group':True,\n",
    "    'beta':0.9,\n",
    "    # initialization\n",
    "    'sparse_init':0.01, 'r':[10.,10.,10.],\n",
    "    # misc\n",
    "    'random_seed':random_seed, 'eval_acc':True,\n",
    "}\n",
    "\n",
    "conf = cf.Conf(**conf_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf24fc71",
   "metadata": {},
   "source": [
    "# Initiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2a15fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {'mean':conf.data_set_mean, 'std':conf.data_set_std}    \n",
    "\n",
    "model = mnist_conv(**model_kwargs)\n",
    "best_model = train.best_model(mnist_conv(**model_kwargs).to(conf.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2696003",
   "metadata": {},
   "source": [
    "# Weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e2e15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {'mean':conf.data_set_mean, 'std':conf.data_set_std}    \n",
    "def init_weights(conf, model):\n",
    "    # sparsify\n",
    "    maf.sparse_bias_uniform_(model, 0,conf.r[0])\n",
    "    maf.sparse_bias_uniform_(model, 0,conf.r[0], ltype=torch.nn.Conv2d)\n",
    "    maf.sparse_weight_normal_(model, conf.r[1])\n",
    "    maf.sparse_weight_normal_(model, conf.r[2], ltype=torch.nn.Conv2d)\n",
    "    #\n",
    "    maf.sparsify_(model, conf.sparse_init, ltype = nn.Conv2d, conv_group=conf.conv_group)\n",
    "    maf.sparsify_(model, conf.sparse_init, ltype = nn.Linear)\n",
    "    model = model.to(conf.device)    \n",
    "    return model\n",
    "\n",
    "model = init_weights(conf,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff0a86b",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51048c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_opt(conf, model):\n",
    "    # -----------------------------------------------------------------------------------\n",
    "    # Get access to different model parameters\n",
    "    # -----------------------------------------------------------------------------------\n",
    "    weights_conv = maf.get_weights_conv(model)\n",
    "    \n",
    "    # Separate first FCC layer from other linear layers\n",
    "    first_fcc = [model.layers2[0].weight]  # First FCC layer (1024→128)\n",
    "    other_linear = [p for n, p in model.named_parameters() \n",
    "                   if isinstance(p, torch.nn.Parameter) and 'weight' in n \n",
    "                   and n != 'layers2.0.weight' and 'layers2' in n]\n",
    "    \n",
    "    biases = maf.get_bias(model)\n",
    "    \n",
    "    # -----------------------------------------------------------------------------------\n",
    "    # Initialize optimizer\n",
    "    # -----------------------------------------------------------------------------------\n",
    "    if conf.conv_group:\n",
    "        reg2 = reg.reg_l1_l2_conv(lamda=conf.lamda_0)\n",
    "    else:\n",
    "        reg2 = reg.reg_l1(lamda=conf.lamda_0)\n",
    "    \n",
    "    if conf.optim == \"SGD\":\n",
    "        opt = torch.optim.SGD(model.parameters(), lr=conf.lr, momentum=conf.beta)\n",
    "    elif conf.optim == \"LinBreg\": # change 'reg' to reg2 if want to use l1_l2 regularization as was previously\n",
    "        opt = op.LinBreg([\n",
    "            {'params': weights_conv, 'lr': conf.lr, 'reg': reg2, 'momentum': conf.beta, 'delta': conf.delta},\n",
    "            # Apply nuclear norm regularization ONLY to first FCC layer\n",
    "            {'params': first_fcc, 'lr': conf.lr, 'reg': DynamicRankNuclearRegularizer(lamda=conf.lamda_1, initial_rank=16, max_rank=96), 'momentum': conf.beta, 'delta': conf.delta},\n",
    "            # Apply L1 regularization to other linear layers\n",
    "            {'params': other_linear, 'lr': conf.lr, 'reg': reg.reg_l1(lamda=conf.lamda_0), 'momentum': conf.beta, 'delta': conf.delta},\n",
    "            {'params': biases, 'lr': conf.lr, 'momentum': conf.beta}\n",
    "        ])\n",
    "    elif conf.optim == \"ProxSGD\":\n",
    "        opt = op.ProxSGD([{'params': weights_conv, 'lr' : conf.lr, 'reg' : reg2, 'momentum':conf.beta,'delta':conf.delta},\n",
    "                          {'params': weights_linear, 'lr' : conf.lr, 'reg' : reg.reg_nuclear_linear(lamda=conf.lamda_1), 'momentum':conf.beta,'delta':conf.delta},\n",
    "                          {'params': biases, 'lr': conf.lr, 'momentum':conf.beta}])            \n",
    "    elif conf.optim == \"AdaBreg\":\n",
    "        opt = op.AdaBreg([{'params': weights_conv, 'lr' : conf.lr, 'reg' : reg.reg_nuclear_conv(lamda=conf.lamda_0),'delta':conf.delta},\n",
    "                           {'params': weights_linear, 'lr' : conf.lr, 'reg' : reg.reg_l1(lamda=conf.lamda_1),'delta':conf.delta},\n",
    "                           {'params': biases, 'lr': conf.lr}])\n",
    "    elif conf.optim == \"L1SGD\":\n",
    "        def weight_reg(model):\n",
    "            reg1 =  reg.reg_l1(lamda=conf.lamda_1)\n",
    "        \n",
    "            loss1 = reg1(model.layers2[0].weight) + reg1(model.layers2[2].weight)\n",
    "            loss2 = reg2(model.layers1[0].weight) + reg2(model.layers1[3].weight)\n",
    "            return loss1 + loss2\n",
    "        \n",
    "        conf.weight_reg = weight_reg\n",
    "        \n",
    "        opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=beta)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown Optimizer specified\")\n",
    "\n",
    "    # learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.5, patience=5,threshold=0.01)\n",
    "    \n",
    "    return opt, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f9836a",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53442c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = ud.get_data_set(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d4ea23",
   "metadata": {},
   "source": [
    "# History and Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985cdc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize history\n",
    "tracked = ['loss', 'node_sparse']\n",
    "train_hist = {}\n",
    "val_hist = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e6adef",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d28eb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------\n",
    "# Reinit weights and the corresponding optimizer\n",
    "# -----------------------------------------------------------------------------------\n",
    "model = init_weights(conf, model)\n",
    "opt, scheduler = init_opt(conf, model) # scheduler is for LR\n",
    "\n",
    "# <<< Instantiate the NuclearLambdaScheduler >>>\n",
    "# Monitor validation loss (mode='min'). Apply to group 1 (weights_linear).\n",
    "# Adjust patience, factor, min_lambda as needed.\n",
    "nuclear_scheduler = op.NuclearLambdaScheduler(\n",
    "    optimizer=opt,\n",
    "    group_idx=1,      # Index of the linear weights group\n",
    "    patience=2,       # Epochs to wait for improvement before reducing lambda\n",
    "    factor=0.5,       # Multiplicative factor for lambda reduction\n",
    "    min_lambda=0.1,  # Minimum lambda value\n",
    "    mode='min',       # Reduce lambda when validation 'loss' stops decreasing\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "rank_scheduler = op.RankScheduler(\n",
    "    optimizer=opt,\n",
    "    group_idx=1,      # Same group as lambda scheduler\n",
    "    patience=3,       # Wait 3 epochs before increasing rank \n",
    "    increase_by=16,   # Increase rank by 16 each time\n",
    "    steps_until_max=5,# Do at most 5 increases (to 16+16*5 = 96)\n",
    "    mode='min',       # Triggered when validation loss plateaus\n",
    "    verbose=True      # Print when rank increases\n",
    ")\n",
    "\n",
    "# Initialize history for tracking metrics\n",
    "effective_rank_histories = {}  # Dictionary to store rank history for each layer\n",
    "test_accuracy_history = []\n",
    "nuclear_lambda_history = [] # <<< Store lambda history >>>\n",
    "rank_history = []  # Initialize rank history list\n",
    "\n",
    "# Initialize timer variables\n",
    "epoch_total_times = [] # List to store total time for each epoch\n",
    "overall_start_time = time.time() # Start timer for all epochs\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# train the model\n",
    "# -----------------------------------------------------------------------------------\n",
    "for epoch in range(conf.epochs):\n",
    "    epoch_start_time = time.time() # Start epoch timer\n",
    "    print(25*\"<>\")\n",
    "    print(50*\"|\")\n",
    "    print(25*\"<>\")\n",
    "    print('Epoch:', epoch)\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # train step, log the accuracy and loss\n",
    "    # ------------------------------------------------------------------------\n",
    "    train_data = train.train_step(conf, model, opt, train_loader)\n",
    "\n",
    "    # update train history\n",
    "    for key in tracked:\n",
    "        if key in train_data:\n",
    "            var_list = train_hist.setdefault(key, [])\n",
    "            var_list.append(train_data[key])\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # validation step\n",
    "    # ------------------------------------------------------------------------\n",
    "    val_data = train.validation_step(conf, model, opt, valid_loader)\n",
    "\n",
    "    # update validation history\n",
    "    for key in tracked:\n",
    "        if key in val_data:\n",
    "            var = val_data[key]\n",
    "            if isinstance(var, list):\n",
    "                for i, var_loc in enumerate(var):\n",
    "                    key_loc = key+\"_\" + str(i)\n",
    "                    var_list = val_hist.setdefault(key_loc, [])\n",
    "                    val_hist[key_loc].append(var_loc)\n",
    "            else:\n",
    "                var_list = val_hist.setdefault(key, [])\n",
    "                var_list.append(var)\n",
    "\n",
    "    # <<< Step the nuclear lambda scheduler using validation loss >>>\n",
    "    val_loss = val_data['loss'] # Or use val_data['acc'] and mode='max'\n",
    "    nuclear_scheduler.step(val_loss)\n",
    "    rank_scheduler.step(val_loss)\n",
    "\n",
    "\n",
    "    # <<< Store current nuclear norm lambda >>>\n",
    "    current_nuclear_lambda = opt.param_groups[1]['reg'].lamda\n",
    "    nuclear_lambda_history.append(current_nuclear_lambda)\n",
    "    print(f\"  Current Nuclear Norm Lambda (Group 1): {current_nuclear_lambda:.2e}\")\n",
    "    # Track the current rank constraint\n",
    "    if epoch == 0:\n",
    "        rank_history = []  # Initialize only once at the start of training\n",
    "    current_rank = opt.param_groups[1]['reg'].get_rank()\n",
    "    rank_history.append(current_rank)\n",
    "\n",
    "    # Track effective rank ratio for each FC layer separately\n",
    "    # Use a consistent epsilon, e.g., 1e-6\n",
    "    fc_layer_ranks = maf.get_linear_layer_ranks(model, epsilon=1e-4)\n",
    "    for layer_name, rank_ratio in fc_layer_ranks.items():\n",
    "        if layer_name not in effective_rank_histories:\n",
    "            effective_rank_histories[layer_name] = []\n",
    "        effective_rank_histories[layer_name].append(rank_ratio)\n",
    "        print(f'  Layer {layer_name} rank ratio (ε=1e-6): {rank_ratio:.4f}')\n",
    "\n",
    "    # Also track test accuracy each epoch\n",
    "    test_data = train.test(conf, model, test_loader, verbosity=0)\n",
    "    test_accuracy_history.append(test_data['acc'])\n",
    "    print(f'  Test Accuracy: {test_data[\"acc\"]:.4f}')\n",
    "\n",
    "    # Step the learning rate scheduler based on train loss\n",
    "    scheduler.step(train_data['loss'])\n",
    "    print(f\"  Learning rate: {opt.param_groups[0]['lr']}\") # Print LR for group 0\n",
    "    best_model(train_data['acc'], val_data['acc'], model=model)\n",
    "\n",
    "    epoch_end_time = time.time() # End epoch timer\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "    epoch_total_times.append(epoch_duration) # Store total epoch duration\n",
    "    print(f\"Epoch {epoch} total time: {epoch_duration:.2f} seconds\") # Print total epoch time\n",
    "\n",
    "\n",
    "overall_end_time = time.time() # End timer for all epochs\n",
    "total_training_time = overall_end_time - overall_start_time # Calculate total time directly\n",
    "\n",
    "# Print final timing information\n",
    "print(\"\\n\" + 50*\"-\")\n",
    "if epoch_total_times: # Avoid division by zero if epochs=0\n",
    "    avg_epoch_time = sum(epoch_total_times) / len(epoch_total_times)\n",
    "    print(f\"Average Total Epoch Time: {avg_epoch_time:.2f} seconds\")\n",
    "print(f\"Total Training Time ({conf.epochs} epochs): {total_training_time:.2f} seconds\")\n",
    "print(50*\"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabbb46d-724c-4d45-a7b3-c5bfef8a3f03",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7c82a0-6940-4c33-bcb2-c4b2ab56171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.test(conf, best_model.best_model, test_loader) \n",
    "print(f'Convolution kernel sparsity: {maf.conv_sparsity(best_model.best_model)}')\n",
    "print(f'Linear sparsity: {maf.linear_sparsity(best_model.best_model)}')\n",
    "linear_rank_ratio = maf.linear_effective_rank_ratio(best_model.best_model, epsilon=1e-4)\n",
    "print(f'Linear Layer Effective Rank Ratio (ε=1e-3): {linear_rank_ratio}')\n",
    "fc_layer_ranks = maf.get_linear_layer_ranks(best_model.best_model, epsilon=1e-4)\n",
    "for layer_name, rank_ratio in fc_layer_ranks.items():\n",
    "    print(f'Layer {layer_name} rank ratio (ε=1e-3): {fc_layer_ranks[layer_name]}')\n",
    "print(\"\\nSingular values of linear layers:\")\n",
    "for name, m in best_model.best_model.named_modules():\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        _, S, _ = torch.svd(m.weight, some=True)\n",
    "        print(f\"Layer {name} singular values: {S[:10]}\")  # Show first 10 values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2fc4af",
   "metadata": {},
   "source": [
    "# Setup plots and appearance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85994ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.style.use('ggplot')\n",
    "matplotlib.rcParams['mathtext.fontset'] = 'cm'\n",
    "matplotlib.rcParams['font.family'] = 'STIXGeneral'\n",
    "matplotlib.rcParams['font.size']=8\n",
    "matplotlib.rcParams['lines.linewidth'] = 1\n",
    "matplotlib.rcParams['lines.markersize'] = 2\n",
    "matplotlib.rcParams['text.color'] = 'black'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af623214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18 (plotting cell)\n",
    "\n",
    "# Create a figure with 1x4 subplots (rank ratio, accuracy, lambda, rank)\n",
    "fig, ax = plt.subplots(1, 4, figsize=(16, 4)) # Adjusted figsize\n",
    "ax = np.ravel(ax)  # Convert to 1D array for consistent indexing\n",
    "\n",
    "# Create arrays for plotting\n",
    "x = np.array(range(conf.epochs))\n",
    "\n",
    "# Dictionary to map layer names to more descriptive labels\n",
    "layer_display_names = {\n",
    "    'layers2.0': 'FCC 1 (1024→128)',  # First fully connected layer\n",
    "    'layers2.2': 'FCC 2 (128→10)'     # Second fully connected layer\n",
    "}\n",
    "\n",
    "# --- Subplot 0: Effective Rank Ratio ---\n",
    "colors = ['blue', 'green', 'purple', 'orange', 'brown', 'pink']\n",
    "handles0, labels0 = [], []\n",
    "for i, (layer_name, history) in enumerate(effective_rank_histories.items()):\n",
    "    color = colors[i % len(colors)]\n",
    "    display_name = layer_display_names.get(layer_name, layer_name)\n",
    "    history_percent = [val * 100 for val in history]\n",
    "    line, = ax[0].plot(x, history_percent, linestyle='-', color=color, label=f'{display_name} Rank Ratio')\n",
    "    ax[0].scatter(x, history_percent, color=color, s=20, alpha=0.5)\n",
    "    handles0.append(line)\n",
    "    labels0.append(f'{display_name} Rank Ratio')\n",
    "\n",
    "ax[0].set_ylabel('Effective Rank Ratio [%]')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].grid(True, linestyle='--', alpha=0.7)\n",
    "ax[0].set_title('Effective Rank Ratio (ε=1e-4)')\n",
    "\n",
    "# --- Subplot 1: Test Accuracy ---\n",
    "test_accuracy_percent = [acc * 100 for acc in test_accuracy_history]\n",
    "line_acc, = ax[1].plot(x, test_accuracy_percent, linestyle='--', color='red', label='Test Accuracy')\n",
    "ax[1].scatter(x, test_accuracy_percent, color='red', s=20, alpha=0.5)\n",
    "ax[1].set_ylabel('Test Accuracy [%]')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].grid(True, linestyle='--', alpha=0.7)\n",
    "ax[1].set_title('Test Accuracy')\n",
    "handles1, labels1 = [line_acc], ['Test Accuracy']\n",
    "\n",
    "# --- Subplot 2: Nuclear Norm Lambda ---\n",
    "line_lambda, = ax[2].plot(x, nuclear_lambda_history, linestyle=':', color='purple', label='Nuclear λ (Group 1)')\n",
    "ax[2].scatter(x, nuclear_lambda_history, color='purple', s=20, alpha=0.5)\n",
    "ax[2].set_ylabel('Nuclear Norm Lambda')\n",
    "ax[2].set_xlabel('Epoch')\n",
    "ax[2].set_yscale('log') # Use log scale for lambda\n",
    "ax[2].grid(True, which=\"both\", linestyle='--', alpha=0.7) # Grid for log scale\n",
    "ax[2].set_title('Nuclear Norm Lambda Schedule')\n",
    "handles2, labels2 = [line_lambda], ['Nuclear λ (Group 1)']\n",
    "\n",
    "# --- Subplot 3: Rank Constraint ---\n",
    "line_rank, = ax[3].plot(x, rank_history, linestyle='-.', color='orange', label='Rank Constraint')\n",
    "ax[3].scatter(x, rank_history, color='orange', s=20, alpha=0.5)\n",
    "ax[3].set_ylabel('Rank Constraint')\n",
    "ax[3].set_xlabel('Epoch')\n",
    "ax[3].grid(True, linestyle='--', alpha=0.7)\n",
    "ax[3].set_title('Rank Constraint Schedule')\n",
    "handles3, labels3 = [line_rank], ['Rank Constraint']\n",
    "\n",
    "# First define xticks\n",
    "max_epoch = len(x) - 1\n",
    "xticks = [i for i in range(0, max_epoch+1, 5)]\n",
    "if max_epoch not in xticks:\n",
    "    xticks.append(max_epoch)\n",
    "\n",
    "# Then apply to all 4 subplots\n",
    "for i in range(4):\n",
    "    ax[i].set_xticks(xticks)\n",
    "\n",
    "# Combine all handles and labels including the rank constraint\n",
    "all_handles = handles0 + handles1 + handles2 + handles3\n",
    "all_labels = labels0 + labels1 + labels2 + labels3\n",
    "\n",
    "# Create legend with all plots\n",
    "fig.legend(all_handles, all_labels, loc='upper left', bbox_to_anchor=(1.01, 1), prop={'size': 8}, ncol=1)\n",
    "\n",
    "# Adjust size and title\n",
    "fig.suptitle('LinBreg with Scheduled Nuclear Norm on FCC Layers', fontsize=10, y=1.02)\n",
    "\n",
    "# Adjust layout to make space for the legend\n",
    "plt.tight_layout(rect=[0, 0, 0.88, 0.98])  # Adjust right margin for legend\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
