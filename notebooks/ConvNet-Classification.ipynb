{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044d7fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various torch packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# ------------------------\n",
    "# get up one directory \n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "# ------------------------\n",
    "\n",
    "# custom packages\n",
    "import models.aux_funs as maf\n",
    "import optimizers as op\n",
    "import regularizers as reg\n",
    "import train\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import utils.configuration as cf\n",
    "import utils.datasets as ud\n",
    "from models.mnist_conv import mnist_conv\n",
    "from scipy.interpolate import make_interp_spline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febb1290",
   "metadata": {},
   "source": [
    "# Fix the random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf73f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 0\n",
    "cf.seed_torch(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce1cf2a",
   "metadata": {},
   "source": [
    "# Configure the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3dd52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_args = {#\n",
    "    # data specification\n",
    "    'data_file':\"../../datasets\",'train_split':0.95, 'data_set':\"Fashion-MNIST\", 'download':True,\n",
    "    # cuda\n",
    "    'use_cuda':False, 'num_workers':2, 'cuda_device':0, 'pin_memory':True, 'train_split':0.95,\n",
    "    #\n",
    "    'epochs':50,\n",
    "    # optimizer\n",
    "    'delta':1.0, 'lr':0.1, 'lamda_0':1e-4, 'lamda_1':0.008, 'optim':\"LinBreg\", 'conv_group':True,\n",
    "    'beta':0.9,\n",
    "    # initialization\n",
    "    'sparse_init':0.01, 'r':[10.,10.,10.],\n",
    "    # misc\n",
    "    'random_seed':random_seed, 'eval_acc':True,\n",
    "}\n",
    "\n",
    "conf = cf.Conf(**conf_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf24fc71",
   "metadata": {},
   "source": [
    "# Initiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2a15fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {'mean':conf.data_set_mean, 'std':conf.data_set_std}    \n",
    "\n",
    "model = mnist_conv(**model_kwargs)\n",
    "best_model = train.best_model(mnist_conv(**model_kwargs).to(conf.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2696003",
   "metadata": {},
   "source": [
    "# Weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e2e15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {'mean':conf.data_set_mean, 'std':conf.data_set_std}    \n",
    "def init_weights(conf, model):\n",
    "    # sparsify\n",
    "    maf.sparse_bias_uniform_(model, 0,conf.r[0])\n",
    "    maf.sparse_bias_uniform_(model, 0,conf.r[0], ltype=torch.nn.Conv2d)\n",
    "    maf.sparse_weight_normal_(model, conf.r[1])\n",
    "    maf.sparse_weight_normal_(model, conf.r[2], ltype=torch.nn.Conv2d)\n",
    "    #\n",
    "    maf.sparsify_(model, conf.sparse_init, ltype = nn.Conv2d, conv_group=conf.conv_group)\n",
    "    maf.sparsify_(model, conf.sparse_init, ltype = nn.Linear)\n",
    "    model = model.to(conf.device)    \n",
    "    return model\n",
    "\n",
    "model = init_weights(conf,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff0a86b",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51048c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_opt(conf, model):\n",
    "    # -----------------------------------------------------------------------------------\n",
    "    # Get access to different model parameters\n",
    "    # -----------------------------------------------------------------------------------\n",
    "    weights_conv = maf.get_weights_conv(model)\n",
    "    weights_linear = maf.get_weights_linear(model)\n",
    "    biases = maf.get_bias(model)\n",
    "    \n",
    "    # -----------------------------------------------------------------------------------\n",
    "    # Initialize optimizer\n",
    "    # -----------------------------------------------------------------------------------\n",
    "    if conf.conv_group:\n",
    "        reg2 = reg.reg_l1_l2_conv(lamda=conf.lamda_0)\n",
    "    else:\n",
    "        reg2 = reg.reg_l1(lamda=conf.lamda_0)\n",
    "    \n",
    "    if conf.optim == \"SGD\":\n",
    "        opt = torch.optim.SGD(model.parameters(), lr=conf.lr, momentum=conf.beta)\n",
    "    elif conf.optim == \"LinBreg\": # change 'reg' to reg2 if want to use l1_l2 regularization as was previously\n",
    "        opt = op.NuclearLinBreg([{'params': weights_conv, 'lr' : conf.lr, 'reg' : reg2, 'momentum':conf.beta,'delta':conf.delta},\n",
    "                          # apply nuclear regularization to the conv layers, switch to l1 reg.reg_l1(lamda=conf.lamda_1) if needed\n",
    "                          {'params': weights_linear, 'lr' : conf.lr, 'reg' : reg.reg_nuclear_linear_truncated(lamda=conf.lamda_1, rank=64), 'momentum':conf.beta,'delta':conf.delta},\n",
    "                          {'params': biases, 'lr': conf.lr, 'momentum':conf.beta}])\n",
    "    elif conf.optim == \"ProxSGD\":\n",
    "        opt = op.ProxSGD([{'params': weights_conv, 'lr' : conf.lr, 'reg' : reg2, 'momentum':conf.beta,'delta':conf.delta},\n",
    "                          {'params': weights_linear, 'lr' : conf.lr, 'reg' : reg.reg_nuclear_linear(lamda=conf.lamda_1), 'momentum':conf.beta,'delta':conf.delta},\n",
    "                          {'params': biases, 'lr': conf.lr, 'momentum':conf.beta}])            \n",
    "    elif conf.optim == \"AdaBreg\":\n",
    "        opt = op.AdaBreg([{'params': weights_conv, 'lr' : conf.lr, 'reg' : reg.reg_nuclear_conv(lamda=conf.lamda_0),'delta':conf.delta},\n",
    "                           {'params': weights_linear, 'lr' : conf.lr, 'reg' : reg.reg_l1(lamda=conf.lamda_1),'delta':conf.delta},\n",
    "                           {'params': biases, 'lr': conf.lr}])\n",
    "    elif conf.optim == \"L1SGD\":\n",
    "        def weight_reg(model):\n",
    "            reg1 =  reg.reg_l1(lamda=conf.lamda_1)\n",
    "        \n",
    "            loss1 = reg1(model.layers2[0].weight) + reg1(model.layers2[2].weight)\n",
    "            loss2 = reg2(model.layers1[0].weight) + reg2(model.layers1[3].weight)\n",
    "            return loss1 + loss2\n",
    "        \n",
    "        conf.weight_reg = weight_reg\n",
    "        \n",
    "        opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=beta)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown Optimizer specified\")\n",
    "\n",
    "    # learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.5, patience=5,threshold=0.01)\n",
    "    \n",
    "    return opt, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f9836a",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53442c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = ud.get_data_set(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d4ea23",
   "metadata": {},
   "source": [
    "# History and Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985cdc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize history\n",
    "tracked = ['loss', 'node_sparse']\n",
    "train_hist = {}\n",
    "val_hist = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e6adef",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d28eb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add at the top of your training cell\n",
    "import time\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# Reinit weights and the corresponding optimizer\n",
    "# -----------------------------------------------------------------------------------\n",
    "model = init_weights(conf, model)\n",
    "opt, scheduler = init_opt(conf, model)\n",
    "\n",
    "# Initialize history for tracking both metrics\n",
    "effective_rank_histories = {}  # Dictionary to store rank history for each layer\n",
    "test_accuracy_history = []\n",
    "\n",
    "# Initialize timer variables\n",
    "total_training_time = 0.0\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# train the model\n",
    "# -----------------------------------------------------------------------------------\n",
    "for epoch in range(conf.epochs):\n",
    "    print(25*\"<>\")\n",
    "    print(50*\"|\")\n",
    "    print(25*\"<>\")\n",
    "    print('Epoch:', epoch)\n",
    "    \n",
    "    # Start timer for this epoch\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # train step, log the accuracy and loss\n",
    "    # ------------------------------------------------------------------------\n",
    "    train_data = train.train_step(conf, model, opt, train_loader)\n",
    "\n",
    "    # update history\n",
    "    for key in tracked:\n",
    "        if key in train_data:\n",
    "            var_list = train_hist.setdefault(key, [])\n",
    "            var_list.append(train_data[key])           \n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # validation step\n",
    "    # ------------------------------------------------------------------------\n",
    "    val_data = train.validation_step(conf, model, opt, valid_loader)\n",
    "\n",
    "    # Calculate and record time for this epoch\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    total_training_time += epoch_time\n",
    "    \n",
    "    print(f\"Epoch {epoch} training time: {epoch_time:.2f} seconds\")\n",
    "    print(f\"Total training time so far: {total_training_time:.2f} seconds\")\n",
    "    \n",
    "    # Rest of your code for tracking metrics (this doesn't count toward training time)\n",
    "    # update validation history\n",
    "    for key in tracked:\n",
    "        if key in val_data:\n",
    "            var = val_data[key]\n",
    "            if isinstance(var, list):\n",
    "                for i, var_loc in enumerate(var):\n",
    "                    key_loc = key+\"_\" + str(i)\n",
    "                    var_list = val_hist.setdefault(key_loc, [])\n",
    "                    val_hist[key_loc].append(var_loc)\n",
    "            else:\n",
    "                var_list = val_hist.setdefault(key, [])\n",
    "                var_list.append(var)   \n",
    "\n",
    "    # Track effective rank ratio for each FC layer separately\n",
    "    fc_layer_ranks = maf.get_linear_layer_ranks(model, epsilon=1e-3)\n",
    "    for layer_name, rank_ratio in fc_layer_ranks.items():\n",
    "        if layer_name not in effective_rank_histories:\n",
    "            effective_rank_histories[layer_name] = []\n",
    "        effective_rank_histories[layer_name].append(rank_ratio)\n",
    "        print(f'Layer {layer_name} rank ratio (ε=1e-3): {rank_ratio:.4f}')\n",
    "    \n",
    "    # Also track test accuracy each epoch\n",
    "    test_data = train.test(conf, model, test_loader, verbosity=0)\n",
    "    test_accuracy_history.append(test_data['acc'])\n",
    "    print(f'Test Accuracy: {test_data[\"acc\"]:.4f}')\n",
    "\n",
    "    scheduler.step(train_data['loss'])\n",
    "    print(\"Learning rate:\", opt.param_groups[0]['lr'])\n",
    "    best_model(train_data['acc'], val_data['acc'], model=model)\n",
    "\n",
    "# Print final timing information\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Final total training time: {total_training_time:.2f} seconds\")\n",
    "print(f\"Average time per epoch: {total_training_time/conf.epochs:.2f} seconds\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabbb46d-724c-4d45-a7b3-c5bfef8a3f03",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7c82a0-6940-4c33-bcb2-c4b2ab56171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.test(conf, best_model.best_model, test_loader) \n",
    "print(f'Convolution kernel sparsity: {maf.conv_sparsity(best_model.best_model)}')\n",
    "print(f'Linear sparsity: {maf.linear_sparsity(best_model.best_model)}')\n",
    "linear_rank_ratio = maf.linear_effective_rank_ratio(best_model.best_model, epsilon=1e-3)\n",
    "print(f'Linear Layer Effective Rank Ratio (ε=1e-3): {linear_rank_ratio}')\n",
    "fc_layer_ranks = maf.get_linear_layer_ranks(best_model.best_model, epsilon=1e-3)\n",
    "for layer_name, rank_ratio in fc_layer_ranks.items():\n",
    "    print(f'Layer {layer_name} rank ratio (ε=1e-3): {fc_layer_ranks[layer_name]}')\n",
    "print(\"\\nSingular values of linear layers:\")\n",
    "for name, m in best_model.best_model.named_modules():\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        _, S, _ = torch.svd(m.weight, some=True)\n",
    "        print(f\"Layer {name} singular values: {S[:10]}\")  # Show first 10 values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2fc4af",
   "metadata": {},
   "source": [
    "# Setup plots and appearance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85994ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.style.use('ggplot')\n",
    "matplotlib.rcParams['mathtext.fontset'] = 'cm'\n",
    "matplotlib.rcParams['font.family'] = 'STIXGeneral'\n",
    "matplotlib.rcParams['font.size']=8\n",
    "matplotlib.rcParams['lines.linewidth'] = 1\n",
    "matplotlib.rcParams['lines.markersize'] = 2\n",
    "matplotlib.rcParams['text.color'] = 'black'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af623214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with 1x2 subplots (rank ratio and accuracy)\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax = np.ravel(ax)  # Convert to 1D array for consistent indexing\n",
    "\n",
    "# Create arrays for plotting\n",
    "x = np.array(range(conf.epochs))\n",
    "\n",
    "# Dictionary to map layer names to more descriptive labels\n",
    "layer_display_names = {\n",
    "    'layers2.0': 'FCC 1 (1024→128)',  # First fully connected layer\n",
    "    'layers2.2': 'FCC 2 (128→10)'     # Second fully connected layer\n",
    "}\n",
    "\n",
    "# Plot effective rank ratio for each layer in first subplot - different colors\n",
    "colors = ['blue', 'green', 'purple', 'orange', 'brown', 'pink']\n",
    "for i, (layer_name, history) in enumerate(effective_rank_histories.items()):\n",
    "    color = colors[i % len(colors)]\n",
    "    # Get descriptive name or use original if not in the dictionary\n",
    "    display_name = layer_display_names.get(layer_name, layer_name)\n",
    "    # Convert to percentage\n",
    "    history_percent = [val * 100 for val in history]\n",
    "    ax[0].plot(x, history_percent, linestyle='-', color=color, label=f'{display_name} Rank Ratio')\n",
    "    ax[0].scatter(x, history_percent, color=color, s=20, alpha=0.5)\n",
    "\n",
    "# Plot test accuracy in second subplot - as before\n",
    "# Convert to percentage\n",
    "test_accuracy_percent = [acc * 100 for acc in test_accuracy_history]\n",
    "ax[1].plot(x, test_accuracy_percent, linestyle='--', color='red', label='Test Accuracy')\n",
    "ax[1].scatter(x, test_accuracy_percent, color='red', s=20, alpha=0.5)\n",
    "\n",
    "# Specify axes\n",
    "## Effective Rank Ratio\n",
    "ax[0].set_ylabel('Effective Rank Ratio [%]')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "## Test Accuracy\n",
    "ax[1].set_ylabel('Test Accuracy [%]')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Modify x-axis to show labels at multiples of 5 for both subplots\n",
    "max_epoch = len(x) - 1\n",
    "xticks = [i for i in range(0, max_epoch+1, 5)]\n",
    "if max_epoch not in xticks:\n",
    "    xticks.append(max_epoch)\n",
    "ax[0].set_xticks(xticks)\n",
    "ax[1].set_xticks(xticks)\n",
    "\n",
    "# Get legend handles from both plots\n",
    "handles = []\n",
    "labels = []\n",
    "for i in range(len(ax)):\n",
    "    h, l = ax[i].get_legend_handles_labels()\n",
    "    handles.extend(h)\n",
    "    labels.extend(l)\n",
    "\n",
    "# Add a legend to the first subplot but position it outside\n",
    "ax[0].legend(handles, labels, loc='upper left', bbox_to_anchor=(1.05, 1), prop={'size': 6}, ncol=1)\n",
    "\n",
    "# Adjust size and title\n",
    "width = 5.50107 / 0.8\n",
    "height = 8.02778 / (2.0)\n",
    "fig.set_size_inches(width, height)\n",
    "fig.suptitle('LinBreg with Nuclear Norm on FCC Layers', fontsize=10)\n",
    "\n",
    "# Adjust layout to make space for the legend\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 0.95])  # Adjust for legend space\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
